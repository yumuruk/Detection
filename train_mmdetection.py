import argparse
import os
import torch
from mmengine.config import Config
from mmengine.runner import Runner
from mmdet.utils import setup_cache_size_limit_of_dynamo

def parse_args():
    parser = argparse.ArgumentParser(description='Train a detector with Auto-Scaling LR')
    parser.add_argument('--config', default='my_configs/DETR.py', help='train config file path')
    parser.add_argument('--work-dir', help='the dir to save logs and models')
    parser.add_argument('--batch-size', type=int, default='8', help='Batch size per GPU (Overwrites config)')
    parser.add_argument('--resume', default=False, action='store_true', help='Resume from latest checkpoint')
    args = parser.parse_args()
    return args

def get_model_settings(config_name):
    """
    Config íŒŒì¼ ì´ë¦„ì— ë”°ë¼ ëª¨ë¸ë³„ Default Settingì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    ë°˜í™˜ê°’: (Optimizerìœ í˜•, Base_LR, Base_Batch_Size, Weight_Decay)
    """
    config_name = config_name.lower()

    # 1. YOLOX ê³„ì—´ (ê°€ì¥ ë¨¼ì € ì²´í¬, ì„¤ì •ì´ ê¹Œë‹¤ë¡œì›€)
    if 'yolox' in config_name:
        print("âš”ï¸ Model detected: YOLOX")
        return {
            'type': 'YOLOX_SGD', # Main í•¨ìˆ˜ì—ì„œ êµ¬ë¶„ì„ ìœ„í•´ ë³„ë„ íƒ€ì… ì§€ì •
            'base_lr': 0.01,     # YOLOX Standard (Batch 64 ê¸°ì¤€)
            'base_batch': 64,    # YOLOXëŠ” ë³´í†µ 8 GPU x 8 samples = 64 ê¸°ì¤€
            'weight_decay': 5e-4,
            'nesterov': True
        }

    # 2. DETR ê³„ì—´ (AdamW ì‚¬ìš©, ë§¤ìš° ë‚®ì€ LR)
    elif 'detr' in config_name or 'dino' in config_name:
        print("ğŸ¤– Model detected: DETR/Transformer-based")
        return {
            'type': 'AdamW',
            'base_lr': 0.0001,  # 1e-4
            'base_batch': 16,   # DETR í‘œì¤€ ë°°ì¹˜
            'weight_decay': 0.0001
        }
    
    # 3. SSD ê³„ì—´ (SGD ì‚¬ìš©, ë³´í†µ LRì´ ì¡°ê¸ˆ ë‚®ìŒ)
    elif 'ssd' in config_name:
        print("ğŸš€ Model detected: SSD")
        return {
            'type': 'SGD',
            'base_lr': 0.001,   # SSDëŠ” ì„¤ì •ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ ë³´í†µ 1e-3 ~ 1e-2
            'base_batch': 32,   # SSD í‘œì¤€ ë°°ì¹˜
            'weight_decay': 5e-4
        }
    
    # 4. Faster R-CNN / RetinaNet ë“± ì¼ë°˜ CNN (SGD ì‚¬ìš©, ë†’ì€ LR)
    else:
        print("ğŸ“¦ Model detected: CNN-based (Faster R-CNN/YOLOv3 etc.)")
        return {
            'type': 'SGD',
            'base_lr': 0.02,    # Standard ImageNet Pretrained LR
            'base_batch': 16,   # MMDetection Standard
            'weight_decay': 0.0001
        }

def main():
    args = parse_args()

    # 1. Config ë¡œë“œ
    cfg = Config.fromfile(args.config)
    
    # 2. Work Directory ì„¤ì •
    if args.work_dir is not None:
        cfg.work_dir = args.work_dir
    elif cfg.get('work_dir', None) is None:
        cfg.work_dir = f'./work_dirs/{os.path.splitext(os.path.basename(args.config))[0]}'

    # 3. GPU ê°œìˆ˜ í™•ì¸ ë° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê³„ì‚°
    num_gpus = torch.cuda.device_count()
    if num_gpus == 0:
        raise RuntimeError("No GPU found. This script requires GPU.")
    
    # ì‚¬ìš©ì ì…ë ¥ Batch Sizeê°€ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°
    if args.batch_size is not None:
        cfg.train_dataloader.batch_size = args.batch_size
    
    per_gpu_batch = cfg.train_dataloader.batch_size
    total_batch_size = per_gpu_batch * num_gpus
    
    # 4. ëª¨ë¸ë³„ ìµœì  ì„¤ì • ê°€ì ¸ì˜¤ê¸° (í•µì‹¬ ë¡œì§)
    settings = get_model_settings(os.path.basename(args.config))
    
    # 5. Linear Scaling Rule ì ìš©
    # New LR = Base LR * (Total Batch Size / Base Batch Size)
    scaling_factor = total_batch_size / settings['base_batch']
    scaled_lr = settings['base_lr'] * scaling_factor

    print("="*50)
    print(f"ğŸ“Š Auto-Scaling Configuration Report")
    print(f"   - GPU Count       : {num_gpus}")
    print(f"   - Per GPU Batch   : {per_gpu_batch}")
    print(f"   - Total Batch Size: {total_batch_size}")
    print(f"   - Model Type      : {settings['type']}")
    print(f"   - Base LR         : {settings['base_lr']} (at batch {settings['base_batch']})")
    print(f"   - Scaling Factor  : x{scaling_factor:.2f}")
    print(f"   âœ… Final LR       : {scaled_lr:.6f}")
    print("="*50)

    # 6. Configì— Optimizer ë° LR ì ìš©
    
    # [Case A] YOLOX (Nesterov + Paramwise Config í•„ìˆ˜)
    if settings['type'] == 'YOLOX_SGD':
        cfg.optim_wrapper.optimizer = dict(
            type='SGD',
            lr=scaled_lr,
            momentum=0.9,
            weight_decay=settings['weight_decay'],
            nesterov=True
        )
        # YOLOX í•µì‹¬: Normê³¼ Biasì—ëŠ” Weight Decay ì ìš© ì•ˆ í•¨
        cfg.optim_wrapper.paramwise_cfg = dict(
            norm_decay_mult=0., 
            bias_decay_mult=0.
        )

    # [Case B] DETR / Transformer (AdamW + Gradient Clip)
    elif settings['type'] == 'AdamW':
        cfg.optim_wrapper.optimizer = dict(
            type='AdamW',
            lr=scaled_lr,
            weight_decay=settings['weight_decay']
        )
        # DETRì€ clip_gradê°€ í•„ìˆ˜
        if cfg.get('optim_wrapper', {}).get('clip_grad', None) is None:
             cfg.optim_wrapper.clip_grad = dict(max_norm=0.1, norm_type=2)

    # [Case C] ì¼ë°˜ CNN (Standard SGD)
    else: 
        cfg.optim_wrapper.optimizer = dict(
            type='SGD',
            lr=scaled_lr,
            momentum=0.9,
            weight_decay=settings['weight_decay']
        )

    # 7. Resume ì„¤ì •
    if args.resume:
        cfg.resume = True

    # 8. Runner ì‹¤í–‰
    setup_cache_size_limit_of_dynamo()
    runner = Runner.from_cfg(cfg)
    runner.train()

if __name__ == '__main__':
    main()