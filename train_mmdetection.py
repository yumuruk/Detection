import argparse
import os
import torch
from mmengine.config import Config
from mmengine.runner import Runner
from mmdet.utils import setup_cache_size_limit_of_dynamo


DATASET_REGISTRY = {
    'UJED': {
        'root': 'data/UJED/', # ê¸°ë³¸ ê²½ë¡œ (Config íŒŒì¼ì˜ data_rootë¥¼ ë®ì–´ì”€)
        'classes': ('echinus', 'holothurian', 'scallop', 'starfish'),
        'palette': [(4, 42, 255), (11, 219, 235), (243, 243, 243), (0, 223, 183)]
    }
}

def parse_args():
    parser = argparse.ArgumentParser(description='Train a detector with Auto-Scaling LR')
    parser.add_argument('--dataset', default='UJED', choices=DATASET_REGISTRY.keys(), help='Select dataset metadata (e.g., classes)')
    parser.add_argument('--data-root', type=str, default=None, help='Override data root path (e.g., data/UJED_CycleGAN/)')
    parser.add_argument('--config', default='my_configs/DETR.py', help='train config file path')
    parser.add_argument('--work-dir', help='the dir to save logs and models')
    parser.add_argument('--batch-size', type=int, default='8', help='Batch size per GPU (Overwrites config)')
    parser.add_argument('--resume', default=False, action='store_true', help='Resume from latest checkpoint')
    args = parser.parse_args()
    return args

def apply_dataset_to_cfg(cfg, dataset_name, override_root=None):
    """Config ê°ì²´ì— ì„ íƒëœ ë°ì´í„°ì…‹ì˜ ê²½ë¡œì™€ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ê°•ì œ ì£¼ì…"""
    if dataset_name not in DATASET_REGISTRY:
        raise ValueError(f"Dataset '{dataset_name}' not found in registry!")
    
    info = DATASET_REGISTRY[dataset_name]
    
    # [í•µì‹¬] ì‹¤ì œ ì‚¬ìš©í•  ë£¨íŠ¸ ê²½ë¡œ ê²°ì •
    real_root = override_root if override_root is not None else info['root']
    if not real_root.endswith('/'):
        real_root += '/'

    classes = info['classes']
    num_classes = len(classes)
    
    print(f"\nğŸ”„ [Dataset Setup]")
    print(f"   - Metadata Source : {dataset_name}")
    print(f"   - Actual Data Root: {real_root}") 
    print(f"   - Classes ({num_classes}) : {classes}")

    # 1. Config ë©”íƒ€ë°ì´í„° ì£¼ì…
    cfg.metainfo = {'classes': classes, 'palette': info.get('palette', None)}
    cfg.data_root = real_root

    # 2. ëª¨ë¸ Head í´ë˜ìŠ¤ ìˆ˜ ì¡°ì • (ì´ì „ì— ë…¼ì˜ëœ ë¡œì§)
    if hasattr(cfg.model, 'bbox_head'):
        if hasattr(cfg.model.bbox_head, 'num_classes'):
            cfg.model.bbox_head.num_classes = num_classes
        elif isinstance(cfg.model.bbox_head, list):
             for head in cfg.model.bbox_head:
                 head.num_classes = num_classes
    if hasattr(cfg.model, 'roi_head') and hasattr(cfg.model.roi_head, 'bbox_head'):
        if hasattr(cfg.model.roi_head.bbox_head, 'num_classes'):
            cfg.model.roi_head.bbox_head.num_classes = num_classes

    # 3. Dataloader ê²½ë¡œ ì¬ì„¤ì • (ìŠ¤í¬ë¦°ìƒ· ê¸°ë°˜ ê²½ë¡œ: annotations/json, images/valid)
    
    # Train
    if hasattr(cfg, 'train_dataloader'):
        cfg.train_dataloader.dataset.data_root = real_root
        cfg.train_dataloader.dataset.metainfo = cfg.metainfo
        cfg.train_dataloader.dataset.ann_file = 'annotations/instances_train.json'
        cfg.train_dataloader.dataset.data_prefix = dict(img='images/train/')

    # Val
    if hasattr(cfg, 'val_dataloader'):
        cfg.val_dataloader.dataset.data_root = real_root
        cfg.val_dataloader.dataset.metainfo = cfg.metainfo
        cfg.val_dataloader.dataset.ann_file = 'annotations/instances_val.json'
        cfg.val_dataloader.dataset.data_prefix = dict(img='images/valid/') # valid í´ë”ëª… ë°˜ì˜

    # Test
    if hasattr(cfg, 'test_dataloader'):
        cfg.test_dataloader.dataset.data_root = real_root
        cfg.test_dataloader.dataset.metainfo = cfg.metainfo
        cfg.test_dataloader.dataset.ann_file = 'annotations/instances_test.json'
        cfg.test_dataloader.dataset.data_prefix = dict(img='images/test/')

    # Evaluator
    if hasattr(cfg, 'val_evaluator'):
        cfg.val_evaluator.ann_file = os.path.join(real_root, 'annotations/instances_val.json')
    if hasattr(cfg, 'test_evaluator'):
        cfg.test_evaluator.ann_file = os.path.join(real_root, 'annotations/instances_test.json')

    return cfg

def get_model_settings(config_name):
    """
    Config íŒŒì¼ ì´ë¦„ì— ë”°ë¼ ëª¨ë¸ë³„ Default Settingì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    ë°˜í™˜ê°’: (Optimizerìœ í˜•, Base_LR, Base_Batch_Size, Weight_Decay)
    """
    config_name = config_name.lower()

    # 1. YOLOX ê³„ì—´ (ê°€ì¥ ë¨¼ì € ì²´í¬, ì„¤ì •ì´ ê¹Œë‹¤ë¡œì›€)
    if 'yolox' in config_name:
        print("âš”ï¸ Model detected: YOLOX")
        return {
            'type': 'YOLOX_SGD', # Main í•¨ìˆ˜ì—ì„œ êµ¬ë¶„ì„ ìœ„í•´ ë³„ë„ íƒ€ì… ì§€ì •
            'base_lr': 0.01,     # YOLOX Standard (Batch 64 ê¸°ì¤€)
            'base_batch': 64,    # YOLOXëŠ” ë³´í†µ 8 GPU x 8 samples = 64 ê¸°ì¤€
            'weight_decay': 5e-4,
            'nesterov': True
        }

    # 2. DETR ê³„ì—´ (AdamW ì‚¬ìš©, ë§¤ìš° ë‚®ì€ LR)
    elif 'detr' in config_name or 'dino' in config_name:
        print("ğŸ¤– Model detected: DETR/Transformer-based")
        return {
            'type': 'AdamW',
            'base_lr': 0.0001,  # 1e-4
            'base_batch': 16,   # DETR í‘œì¤€ ë°°ì¹˜
            'weight_decay': 0.0001
        }
    
    # 3. SSD ê³„ì—´ (SGD ì‚¬ìš©, ë³´í†µ LRì´ ì¡°ê¸ˆ ë‚®ìŒ)
    elif 'ssd' in config_name:
        print("ğŸš€ Model detected: SSD")
        return {
            'type': 'SGD',
            'base_lr': 0.001,   # SSDëŠ” ì„¤ì •ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ ë³´í†µ 1e-3 ~ 1e-2
            'base_batch': 32,   # SSD í‘œì¤€ ë°°ì¹˜
            'weight_decay': 5e-4
        }
    
    # 4. Faster R-CNN / RetinaNet ë“± ì¼ë°˜ CNN (SGD ì‚¬ìš©, ë†’ì€ LR)
    else:
        print("ğŸ“¦ Model detected: CNN-based (Faster R-CNN/YOLOv3 etc.)")
        return {
            'type': 'SGD',
            'base_lr': 0.02,    # Standard ImageNet Pretrained LR
            'base_batch': 16,   # MMDetection Standard
            'weight_decay': 0.0001
        }

def main():
    args = parse_args()

    # 1. Config ë¡œë“œ
    cfg = Config.fromfile(args.config)
    
    # [ì¶”ê°€] 3ë‹¨ê³„: ë°ì´í„°ì…‹ ì •ë³´ë¡œ Config ë®ì–´ì“°ê¸°
    cfg = apply_dataset_to_cfg(cfg, args.dataset, override_root=args.data_root)
    
    # 2. Work Directory ì„¤ì • (ë°ì´í„°ì…‹ í´ë” ì´ë¦„ì„ ë¡œê·¸ í´ë”ì— í¬í•¨ì‹œí‚´)
    if args.work_dir is not None:
        cfg.work_dir = args.work_dir
    elif cfg.get('work_dir', None) is None:
        config_name = os.path.splitext(os.path.basename(args.config))[0]
        
        # ì‹¤ì œ ë°ì´í„° í´ë” ì´ë¦„ì„ ë”°ì™€ì„œ ë¡œê·¸ í´ë”ëª…ì— ì‚¬ìš©
        if args.data_root:
            dataset_folder = os.path.basename(os.path.normpath(args.data_root))
        else:
            dataset_folder = args.dataset
            
        cfg.work_dir = f'./work_dirs/{config_name}_{dataset_folder}'

    # 3. GPU ê°œìˆ˜ í™•ì¸ ë° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê³„ì‚° (ê¸°ì¡´ ë¡œì§)
    num_gpus = torch.cuda.device_count()
    if num_gpus == 0:
        raise RuntimeError("No GPU found. This script requires GPU.")
    
    # ì‚¬ìš©ì ì…ë ¥ Batch Sizeê°€ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°
    if args.batch_size is not None:
        cfg.train_dataloader.batch_size = args.batch_size
    
    per_gpu_batch = cfg.train_dataloader.batch_size
    total_batch_size = per_gpu_batch * num_gpus
    
    # 4. ëª¨ë¸ë³„ ìµœì  ì„¤ì • ê°€ì ¸ì˜¤ê¸° (í•µì‹¬ ë¡œì§)
    settings = get_model_settings(os.path.basename(args.config))
    
    # 5. Linear Scaling Rule ì ìš©
    scaling_factor = total_batch_size / settings['base_batch']
    scaled_lr = settings['base_lr'] * scaling_factor

    print("="*50)
    print(f"ğŸ“Š Auto-Scaling Configuration Report")
    print(f" Â  - GPU Count Â  Â  Â  : {num_gpus}")
    print(f" Â  - Per GPU Batch Â  : {per_gpu_batch}")
    print(f" Â  - Total Batch Size: {total_batch_size}")
    print(f" Â  - Model Type Â  Â  Â : {settings['type']}")
    print(f" Â  - Base LR Â  Â  Â  Â  : {settings['base_lr']} (at batch {settings['base_batch']})")
    print(f" Â  - Scaling Factor Â : x{scaling_factor:.2f}")
    print(f" Â  âœ… Final LR Â  Â  Â  : {scaled_lr:.6f}")
    print("="*50)

    # 6. Configì— Optimizer ë° LR ì ìš© (ê¸°ì¡´ ë¡œì§)
    if settings['type'] == 'YOLOX_SGD':
        cfg.optim_wrapper.optimizer = dict(type='SGD', lr=scaled_lr, momentum=0.9, weight_decay=settings['weight_decay'], nesterov=True)
        cfg.optim_wrapper.paramwise_cfg = dict(norm_decay_mult=0., bias_decay_mult=0.)
    elif settings['type'] == 'AdamW':
        cfg.optim_wrapper.optimizer = dict(type='AdamW', lr=scaled_lr, weight_decay=settings['weight_decay'])
        if cfg.get('optim_wrapper', {}).get('clip_grad', None) is None:
             cfg.optim_wrapper.clip_grad = dict(max_norm=0.1, norm_type=2)
    else: 
        cfg.optim_wrapper.optimizer = dict(type='SGD', lr=scaled_lr, momentum=0.9, weight_decay=settings['weight_decay'])

    # 7. Resume ì„¤ì •
    if args.resume:
        cfg.resume = True

    # 8. Runner ì‹¤í–‰
    setup_cache_size_limit_of_dynamo()
    runner = Runner.from_cfg(cfg)
    runner.train()

if __name__ == '__main__':
    main()